{"cells":[{"cell_type":"markdown","source":["## Exploring Delta Lake in Microsoft Fabric: A Relational Perspective\n","**by Jesus Lopez Martin - https://www.syntax.es - December 2024**\n","\n","The Setup: We worked in a Microsoft Fabric Workspace on an F2 capacity, using a Lakehouse with Schemas and a Notebook in PySpark and Scala SQL. Our primary dataset? The \"sales\" 100M table from [Contoso Data Generator](https://github.com/sql-bi/Contoso-Data-Generator-V2-Data/releases/tag/ready-to-use-data). "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64c0bd39-9a2a-415b-9319-5ca297ca6f01"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_date\n","import datetime\n","\n","# Path to Parquet in Azure Blob Storage\n","parquet_path = \"wasbs://<your_container>@<your_storageaccount>.blob.core.windows.net/parquet100m/sales.parquet\"\n","table_name = \"bronze_wwi.sales\"\n","version_mapping_table = \"bronze_wwi.sales_version_mapping\"\n","\n","# Read the Parquet file complet\n","print(f\"Reading data from: {parquet_path}\")\n","sales_df = spark.read.format(\"parquet\").load(parquet_path)\n","\n","# Dates\n","min_date = sales_df.selectExpr(\"MIN(OrderDate)\").collect()[0][0]\n","max_date = sales_df.selectExpr(\"MAX(OrderDate)\").collect()[0][0]\n","print(f\"Range of dates in OrderDate: {min_date} a {max_date}\")\n","\n","start_date = min_date \n","end_date = max_date\n","\n","# List for register map between dates and versions\n","sales_version_mapping = []\n","\n","# Create Delta table day by day\n","current_date = start_date\n","\n","while current_date <= end_date:\n","    # Filter actual day\n","    current_day_str = current_date.strftime(\"%Y-%m-%d\")\n","    daily_data = sales_df.filter(to_date(col(\"OrderDate\")) == current_day_str)\n","\n","    if daily_data.count() > 0: \n","        print(f\"Loading data for day: {current_day_str}, record count: {daily_data.count()}\")\n","\n","        # Register metadata in commit\n","        spark.conf.set(\"spark.databricks.delta.commitInfo.userMetadata\", f\"Simulated day: {current_day_str}\")\n","\n","        # Write data in Delta table\n","        daily_data.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n","\n","        # Last version\n","        history = spark.sql(f\"DESCRIBE HISTORY {table_name}\").collect()\n","        latest_version = history[0]['version']\n","\n","        # Register the date and version in list\n","        sales_version_mapping.append((current_day_str, latest_version))\n","\n","    # Next day\n","    current_date += datetime.timedelta(days=1)\n","\n","# Create a DataFrame with map and save how Delta table\n","mapping_df = spark.createDataFrame(sales_version_mapping, [\"date\", \"version\"])\n","mapping_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(version_mapping_table)\n","\n","print(\"Load complete and map generated.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d60ceb7-ac5f-48c4-be03-5c5bd77b18e3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}